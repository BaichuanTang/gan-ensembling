{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Plot paper graphs using precomputed evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from matplotlib import rc\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics\n",
    "from collections import defaultdict, OrderedDict\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "pd.options.display.float_format = '{:0.2f}'.format\n",
    "rc('font', **{'family': 'serif'})\n",
    "\n",
    "from data import data_celebahq\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p pdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### plot format utilities ### \n",
    "\n",
    "sns.set(style='whitegrid')\n",
    "sns.set_style({'font.family': 'serif'})\n",
    "\n",
    "def save(f, filename, extra_artists=None):\n",
    "    f.savefig(os.path.join('pdfs', filename), bbox_inches='tight', dpi=300, bbox_extra_artists=extra_artists)\n",
    "    \n",
    "def adjust_saturation(palette, s):\n",
    "    new_palette = [sns.set_hls_values(color=p, h=None, l=None, s=s)\n",
    "                   for p in palette]\n",
    "    return new_palette\n",
    "\n",
    "def bar_offset(group_size, n_groups, barwidth):\n",
    "    # utility function to get x-axis values for grouped bar plots\n",
    "    xvals = np.arange(1, n_groups+1)\n",
    "    halfwidth = barwidth / 2\n",
    "    offsets = [i * barwidth for i in range(group_size)]\n",
    "    if group_size % 2 == 1:\n",
    "        middle = offsets[int(len(offsets) / 2)]\n",
    "    if group_size % 2 == 0:\n",
    "        middle = np.mean(offsets[int(len(offsets) / 2)-1:int(len(offsets) / 2)+1])\n",
    "    offsets = [off - middle for off in offsets]\n",
    "    return [xvals + off for off in offsets]\n",
    "\n",
    "def get_list_stats(l):\n",
    "    mean = np.mean(l)\n",
    "    stderr = np.std(l) / np.sqrt(len(l))\n",
    "    n = len(l)\n",
    "    return {'mean': mean, 'stderr': stderr, 'n': n}\n",
    "\n",
    "def make_green_palette(n):\n",
    "    return sns.light_palette([0.39215686, 0.61960784, 0.45098039], n_colors=n)\n",
    "\n",
    "def make_blue_palette(n):\n",
    "    return sns.light_palette([0.29803922, 0.44705882, 0.69019608], n_colors=n)\n",
    "\n",
    "def make_purple_palette(n):\n",
    "    return sns.light_palette([0.5058823529411764, 0.4470588235294118, 0.7019607843137254], n_colors=n)\n",
    "\n",
    "def make_yellow_palette(n):\n",
    "    return sns.light_palette([0.8666666666666667, 0.5176470588235295, 0.3215686274509804], n_colors=n)\n",
    "    \n",
    "def make_diverging_palette(n):\n",
    "    return sns.color_palette(\"vlag\", n_colors=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### data evaluation utilities ### \n",
    "\n",
    "def softmax_to_prediction(softmax_prediction):\n",
    "    # converts softmax prediction to discrete class label\n",
    "    if np.ndim(softmax_prediction) == 2:\n",
    "        # N x ensembles binary prediction\n",
    "        return (softmax_prediction > 0.5).astype(int)\n",
    "    elif np.ndim(softmax_prediction) == 3:\n",
    "        # N x ensembles x classes\n",
    "        return np.argmax(softmax_prediction, axis=-1).squeeze()\n",
    "    else:\n",
    "        assert(False)\n",
    "        \n",
    "def get_accuracy_from_image_ensembles(data_file, key, resample=False, seed=0, \n",
    "                                      n_resamples=20, ens_size=32, verbose=True):\n",
    "    # helper function to extract ensembled accuracy from image augmentations\n",
    "    # e.g. image_ensemble_imcolor.npz or image_ensemble_imcrop.npz\n",
    "    encoded_data = np.load(data_file)\n",
    "    preds_original = softmax_to_prediction(encoded_data['original'])\n",
    "    acc_original = sklearn.metrics.accuracy_score(encoded_data['label'], preds_original) * 100\n",
    "    jitters = np.concatenate([encoded_data['original'], encoded_data[key]], axis=1)\n",
    "    jitters = np.mean(jitters, axis=1, keepdims=True)\n",
    "    preds_ensembled = softmax_to_prediction(jitters)\n",
    "    acc_ensembled = sklearn.metrics.accuracy_score(encoded_data['label'], preds_ensembled) * 100\n",
    "    \n",
    "    resamples = None\n",
    "    if resample:\n",
    "        # sample num_samples batches with replacement, compute accuracy\n",
    "        resamples = []\n",
    "        rng = np.random.RandomState(seed)\n",
    "        jitters = np.concatenate([encoded_data['original'], encoded_data[key]], axis=1)\n",
    "        assert(jitters.shape[1] == ens_size) # sanity check\n",
    "        for i in range(n_resamples):\n",
    "            if verbose:\n",
    "                print('*', end='')\n",
    "            indices = rng.choice(jitters.shape[1], ens_size, replace=True)\n",
    "            jitters_resampled = jitters[:, indices]\n",
    "            jitters_resampled = np.mean(jitters_resampled, axis=1, keepdims=True)\n",
    "            preds_ensembled = softmax_to_prediction(jitters_resampled)\n",
    "            resamples.append(sklearn.metrics.accuracy_score(encoded_data['label'], preds_ensembled) * 100)\n",
    "        if verbose:\n",
    "            print(\"done\")\n",
    "    return {'acc_original': acc_original, 'acc_ensembled': acc_ensembled, 'resamples': resamples}\n",
    "    \n",
    "def sample_ensemble(raw_preds, ens_size=None, seed=None):\n",
    "    # helper function to resample raw ensemble predictions\n",
    "    # raw_preds = N x ens_size for  binary classification, or N x ens_size x classes\n",
    "    # ens_size = number of samples to take preds for ensembling, None takes all all samples\n",
    "    # seed = random seed to use when sampling with replacement, None takes samples in order\n",
    "    if ens_size is None:\n",
    "        ens_size = raw_preds.shape[1] # take all samples\n",
    "    if seed is None:\n",
    "        ensemble_preds = raw_preds[:, range(ens_size)] # take the samples in order\n",
    "    else: # sample the given preds with replacement\n",
    "        rng = np.random.RandomState(seed)\n",
    "        indices = rng.choice(raw_preds.shape[1], ens_size, replace=True)\n",
    "        ensemble_preds = raw_preds[:, indices]\n",
    "    return ensemble_preds\n",
    "\n",
    "def get_accuracy_from_npz(data_file, expt_name, weight=None, ens_size=None, seed=None, return_preds=False,\n",
    "                          add_aug=False, aug_name='image_ensemble_imcrop', aug_key='imcrop'):\n",
    "    # compute weighted accuracies combining original image and GAN reconstructions from an npz_file\n",
    "    # option to use either single original image, or multiple image augmentations for the image views\n",
    "    \n",
    "    # setup\n",
    "    encoded_data = np.load(data_file)\n",
    "    df = defaultdict(list)\n",
    "    expt_settings = os.path.basename(data_file).split('.')[0]\n",
    "    if weight is not None:\n",
    "        weights = [weight]\n",
    "    else:\n",
    "        weights = np.linspace(0, 1, 21)\n",
    "\n",
    "    # determine image classification accuracy\n",
    "    if not add_aug:\n",
    "        # basic case: just load the image predictions from the data file\n",
    "        preds_original = softmax_to_prediction(encoded_data['original'])\n",
    "        original = encoded_data['original'] # full softmax distribution\n",
    "    else:\n",
    "        # ensemble also with the image augmentations data\n",
    "        print('.', end='')\n",
    "        im_aug_data = np.load(os.path.join(data_file.rsplit('/', 1)[0], '%s.npz' % aug_name))\n",
    "        im_aug_ens = np.concatenate([im_aug_data['original'], im_aug_data[aug_key]], axis=1)\n",
    "        im_aug_ens = sample_ensemble(im_aug_ens, ens_size, seed)\n",
    "        im_aug_ens = np.mean(im_aug_ens, axis=1, keepdims=True) \n",
    "        preds_original = softmax_to_prediction(im_aug_ens)\n",
    "        original = im_aug_ens # full softmax distribution\n",
    "    acc_original = sklearn.metrics.accuracy_score(encoded_data['label'], preds_original) * 100\n",
    "    \n",
    "    # determine GAN reconstruction accuracy\n",
    "    preds_reconstructed = softmax_to_prediction(encoded_data['reconstructed'])\n",
    "    acc_reconstructed = sklearn.metrics.accuracy_score(encoded_data['label'], preds_reconstructed) * 100\n",
    "    \n",
    "    # determine GAN ensemble accuracy\n",
    "    perturbed = encoded_data[expt_name] # N x ens_size x softmax distribution\n",
    "    gan_ens = np.concatenate((encoded_data['reconstructed'], perturbed), axis=1)\n",
    "    if ens_size == 0:\n",
    "        gan_ens = original # dummy case: don't use gan reconstructed images\n",
    "    else:\n",
    "        gan_ens = sample_ensemble(gan_ens, ens_size, seed)    \n",
    "    for weight in weights: # alpha weighting hyperparameter\n",
    "        # for binary classification: original.shape = N x 1, gan_ens.shape = N x ens_size\n",
    "        # for multi-class classification: original.shape = N x 1 x classes; gan_ens.shape = N x ens_size x classes\n",
    "        ensembled = (1-weight) * original + weight * np.mean(gan_ens, axis=1, keepdims=True)\n",
    "        preds_ensembled = softmax_to_prediction(ensembled)\n",
    "        acc_ensembled = sklearn.metrics.accuracy_score(encoded_data['label'], preds_ensembled) * 100\n",
    "        df['acc'].append(acc_ensembled)\n",
    "        df['weight'].append(weight)\n",
    "        df['expt_name'].append(expt_name)\n",
    "\n",
    "    # table of expt_name x weight\n",
    "    df = pd.DataFrame.from_dict(df)\n",
    "    return_data = {'expt_settings': expt_settings, \n",
    "                   'acc_original': acc_original,\n",
    "                   'acc_reconstructed': acc_reconstructed,\n",
    "                   'ensemble_table': df}\n",
    "    if return_preds:\n",
    "        assert(len(weights) == 1)\n",
    "        return_preds = {\n",
    "            'original': original, # original softmax\n",
    "            'reconstruction': gan_ens, # softmax of all gan views\n",
    "            'ensembled': ensembled, # softmax of the weighted ensemble\n",
    "            'pred_original': preds_original,\n",
    "            'pred_reconstruction': preds_reconstructed,\n",
    "            'pred_ensemble': preds_ensembled,\n",
    "            'label': encoded_data['label'],\n",
    "        }\n",
    "        return return_data, return_preds\n",
    "    return return_data\n",
    "\n",
    "def compute_best_weight(val_data_file, test_data_file, expt_name, \n",
    "                        verbose=True, ens_size=None, seed=None,\n",
    "                        add_aug=False, aug_name='image_ensemble_imcrop', aug_key='imcrop'):\n",
    "    # given a val data file and a test data file, find the best weighting between \n",
    "    # image view and GAN-generated views on the val split, and use that weighting on the test split\n",
    "    \n",
    "    # sanity checks\n",
    "    assert('val' in val_data_file)\n",
    "    assert('test' in test_data_file)\n",
    "    val_accuracy_info = get_accuracy_from_npz(val_data_file, expt_name, \n",
    "                                              weight=None, ens_size=ens_size, seed=seed,\n",
    "                                              add_aug=add_aug, aug_name=aug_name, aug_key=aug_key)\n",
    "    val_ensemble_table = val_accuracy_info['ensemble_table']\n",
    "    # find the optimal ensemble weight from validation\n",
    "    best_val_setting = val_ensemble_table.iloc[val_ensemble_table['acc'].argsort().iloc[-1], :]\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Val original %0.4f Val reconstructed %0.4f\" % \n",
    "              (val_accuracy_info['acc_original'], val_accuracy_info['acc_reconstructed']))\n",
    "        print(\"%0.4f @ %0.4f %s\" % (best_val_setting['acc'], best_val_setting['weight'], best_val_setting['expt_name']))\n",
    "    \n",
    "    test_accuracy_info = get_accuracy_from_npz(test_data_file, expt_name, \n",
    "                                               weight=best_val_setting['weight'], \n",
    "                                               ens_size=ens_size, seed=seed,\n",
    "                                               add_aug=add_aug, aug_name=aug_name, aug_key=aug_key)\n",
    "    test_ensemble_table = test_accuracy_info['ensemble_table']\n",
    "    assert(test_ensemble_table.shape[0] == 1) # it should only evaluate at the specified weight\n",
    "    test_setting_from_val = test_ensemble_table.iloc[0, :] # gets the single element from the table\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Test original %0.4f Test reconstructed %0.4f\" % \n",
    "              (test_accuracy_info['acc_original'], test_accuracy_info['acc_reconstructed']))\n",
    "        print(\"%0.4f @ %0.4f %s\" % (test_setting_from_val['acc'], test_setting_from_val['weight'],\n",
    "                                    test_setting_from_val['expt_name']))\n",
    "    \n",
    "    return {'val_info': val_accuracy_info, 'test_info': test_accuracy_info, \n",
    "            'val_setting': best_val_setting, 'test_setting': test_setting_from_val}\n",
    "\n",
    "\n",
    "def resample_wrapper(val_file, test_file, expt_name, ens_size, add_aug, n_resamples=20, verbose=False,\n",
    "                     aug_name='image_ensemble_imcrop', aug_key='imcrop'):\n",
    "    # due to randomness in sampling, it helps to sample multiple times and average the results for stability\n",
    "    # this function wraps compute_best_weight(), using the specified ensemble size and resampling multiple times\n",
    "    \n",
    "    val_samples = []\n",
    "    test_samples = []\n",
    "    weights = []\n",
    "    assert(ens_size==31 or (ens_size==16 and add_aug==True))\n",
    "    # using ens_size=31 so that with the original image, total size=32; or 16 image views and 16 GAN views\n",
    "    for s in range(n_resamples):\n",
    "        res = compute_best_weight(val_file, test_file, expt_name, verbose=verbose, add_aug=add_aug, \n",
    "                                  ens_size=ens_size, seed=s, aug_name=aug_name, aug_key=aug_key)\n",
    "        val_samples.append(res['val_setting']['acc'])\n",
    "        test_samples.append(res['test_setting']['acc'])\n",
    "        weights.append(res['test_setting']['weight'])\n",
    "    return {'val_avg': np.mean(val_samples),\n",
    "            'test_avg': np.mean(test_samples), \n",
    "            'val_stderr': np.std(val_samples) / np.sqrt(n_resamples),\n",
    "            'test_stderr': np.std(test_samples) / np.sqrt(n_resamples),\n",
    "            'weights': weights,\n",
    "            'val_acc_original': res['val_info']['acc_original'],\n",
    "            'test_acc_original': res['test_info']['acc_original'],\n",
    "            'val_acc_rec': res['val_info']['acc_reconstructed'],\n",
    "            'test_acc_rec': res['test_info']['acc_reconstructed'],\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cars domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample 32 crops of images, compare to combination of 16 crops of images and 16 crops of gan\n",
    "df = defaultdict(list)\n",
    "\n",
    "for i, classifier in enumerate(['imageclassifier', 'latentclassifier', \n",
    "                                'latentclassifier_stylemix_fine']):\n",
    "    print(classifier)\n",
    "    val_expts = [\n",
    "        (f'results/precomputed_evaluations/car/output/{classifier}_val/gan_ensemble_isotropic_coarse_tensortransform.npz',\n",
    "         ('isotropic_coarse_1.00', 'isotropic_coarse_1.50', 'isotropic_coarse_2.00'), 'Isotropic Coarse'),\n",
    "        (f'results/precomputed_evaluations/car/output/{classifier}_val/gan_ensemble_isotropic_fine_tensortransform.npz',\n",
    "         ('isotropic_fine_0.30', 'isotropic_fine_0.50', 'isotropic_fine_0.70'), 'Isotropic Fine'),\n",
    "        (f'results/precomputed_evaluations/car/output/{classifier}_val/gan_ensemble_pca_coarse_tensortransform.npz',\n",
    "         ('pca_coarse_1.00', 'pca_coarse_2.00', 'pca_coarse_3.00'), 'PCA Coarse'),\n",
    "        (f'results/precomputed_evaluations/car/output/{classifier}_val/gan_ensemble_pca_fine_tensortransform.npz',\n",
    "         ('pca_fine_1.00', 'pca_fine_2.00', 'pca_fine_3.00'), 'PCA Fine'),\n",
    "#         (f'results/precomputed_evaluations/car/output/{classifier}_val/gan_ensemble_stylemix_coarse_tensortransform.npz',\n",
    "#          ('stylemix_coarse',), 'Style-mix Coarse'),\n",
    "        (f'results/precomputed_evaluations/car/output/{classifier}_val/gan_ensemble_stylemix_fine_tensortransform.npz',\n",
    "         ('stylemix_fine',), 'Style-mix Fine'),\n",
    "    ]\n",
    "    test_expts = [(x.replace('_val/', '_test/'), y, z) for x, y, z in val_expts]\n",
    "\n",
    "    for val, test in zip(val_expts, test_expts):\n",
    "        expt_settings = []\n",
    "        print(val[-1])\n",
    "        for expt_name in val[1]:\n",
    "            resampled_accs = resample_wrapper(val[0], test[0], expt_name, ens_size=16, \n",
    "                                              add_aug=True, aug_name='image_ensemble_imcrop', verbose=False)            \n",
    "            resampled_accs['expt_name'] = expt_name\n",
    "            expt_settings.append(resampled_accs)\n",
    "            print(\"done\")            \n",
    "            \n",
    "        best_expt = max(expt_settings, key=lambda x: x['val_avg']) # take the val accuracy, avged over samples\n",
    "        df['classifier'].append(classifier+'_crop')    \n",
    "        df['acc'].append(best_expt['test_avg'])\n",
    "        df['stderr'].append(best_expt['test_stderr'])\n",
    "        df['expt'].append(best_expt['expt_name'])\n",
    "        df['expt_group'].append(test[2])\n",
    "\n",
    "df = pd.DataFrame.from_dict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot it\n",
    "f, ax = plt.subplots(1, 1, figsize=(7, 5))\n",
    "\n",
    "data_file = f'results/precomputed_evaluations/car/output/imageclassifier_test/image_ensemble_imcrop.npz'\n",
    "im_crops = get_accuracy_from_image_ensembles(data_file, 'imcrop', resample=True)\n",
    "\n",
    "group_size = 5\n",
    "bar_width=0.15\n",
    "n_groups = 3\n",
    "bar_offsets = bar_offset(group_size, n_groups, bar_width)\n",
    "palette = make_blue_palette(3)[1:] + make_green_palette(3)[1:] + make_purple_palette(3)[1:]\n",
    "\n",
    "resample_stats = get_list_stats(im_crops['resamples'])\n",
    "ind = 0.2\n",
    "ax.axhline(im_crops['acc_ensembled'], color='k', linestyle=':', label='Original Images')\n",
    "\n",
    "xticklabels = []\n",
    "for i in range(group_size):\n",
    "    indices = np.arange(i, n_groups*group_size, group_size)\n",
    "    bar_height = df.iloc[indices]['acc']\n",
    "    bar_err = df.iloc[indices]['stderr']\n",
    "    assert(all([x == df.iloc[indices[0]]['expt_group'] for x in df.iloc[indices]['expt_group']]))\n",
    "    ax.bar(bar_offsets[i], bar_height, width=bar_width, color=palette[i], yerr=bar_err,\n",
    "           label=df.iloc[indices[0]]['expt_group'], edgecolor=(0.5, 0.5, 0.5), capsize=5)\n",
    "    xticklabels.append(df.iloc[indices[0]]['classifier'].replace('_', '\\n'))\n",
    "\n",
    "ax.set_ylim([94, 99])\n",
    "ax.set_xticks(list(range(1, n_groups+1)))\n",
    "handles,labels = ax.get_legend_handles_labels()\n",
    "# reorder it so it looks nicer\n",
    "order = [0, 3, 1, 4, 2, 5]\n",
    "handles = [handles[i] for i in order]\n",
    "labels = [labels[i] for i in order]\n",
    "ax.legend(handles, labels, loc='upper center', ncol=3, prop={'size': 11})\n",
    "\n",
    "# ax.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, -0.3), ncol=3, prop={'size': 11})\n",
    "# ax.legend(loc='center left', bbox_to_anchor=(1, 0.5), prop={'size': 14})\n",
    "ax.set_xticklabels(['Original\\nImages', 'GAN\\nRecontructions', 'Style-mix Fine\\nAugmentations'], fontsize=12)\n",
    "ax.set_xlabel('Classifier training distribution', fontsize=16)\n",
    "ax.set_ylabel('Classification Accuracy', fontsize=16)\n",
    "for tick in ax.yaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(14) \n",
    "ax.set_title('Cars', fontsize=16)\n",
    "\n",
    "f.tight_layout()\n",
    "save(f, 'graph_cars_v2.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample 32 crops of images, compare to combination of 16 crops of images and 16 crops of gan\n",
    "# using all experiment settings for supplemental\n",
    "\n",
    "df = defaultdict(list)\n",
    "im_crop_data = []\n",
    "\n",
    "for i, classifier in enumerate(['imageclassifier', 'latentclassifier', \n",
    "                                'latentclassifier_isotropic_fine', 'latentclassifier_isotropic_coarse',\n",
    "                                'latentclassifier_pca_fine', 'latentclassifier_pca_coarse',\n",
    "                                'latentclassifier_stylemix_fine', 'latentclassifier_stylemix_coarse']):\n",
    "    print(classifier)\n",
    "    val_expts = [\n",
    "        (f'results/precomputed_evaluations/car/output/{classifier}_val/gan_ensemble_isotropic_coarse_tensortransform.npz', \n",
    "         ('isotropic_coarse_1.00', 'isotropic_coarse_1.50', 'isotropic_coarse_2.00'), 'Isotropic Coarse'),\n",
    "        (f'results/precomputed_evaluations/car/output/{classifier}_val/gan_ensemble_isotropic_fine_tensortransform.npz', \n",
    "         ('isotropic_fine_0.30', 'isotropic_fine_0.50', 'isotropic_fine_0.70'), 'Isotropic Fine'),\n",
    "        (f'results/precomputed_evaluations/car/output/{classifier}_val/gan_ensemble_pca_coarse_tensortransform.npz', \n",
    "         ('pca_coarse_1.00', 'pca_coarse_2.00', 'pca_coarse_3.00'), 'PCA Coarse'),\n",
    "        (f'results/precomputed_evaluations/car/output/{classifier}_val/gan_ensemble_pca_fine_tensortransform.npz',\n",
    "         ('pca_fine_1.00', 'pca_fine_2.00', 'pca_fine_3.00'), 'PCA Fine'),\n",
    "        (f'results/precomputed_evaluations/car/output/{classifier}_val/gan_ensemble_stylemix_coarse_tensortransform.npz', \n",
    "         ('stylemix_coarse',), 'Style-mix Coarse'),\n",
    "        (f'results/precomputed_evaluations/car/output/{classifier}_val/gan_ensemble_stylemix_fine_tensortransform.npz', \n",
    "         ('stylemix_fine',), 'Style-mix Fine'),\n",
    "    ]\n",
    "\n",
    "    test_expts = [(x.replace('_val/', '_test/'), y, z) for x, y, z in val_expts]\n",
    "    \n",
    "    data_file = f'results/precomputed_evaluations/car/output/{classifier}_test/image_ensemble_imcrop.npz'\n",
    "    im_crop_data.append(get_accuracy_from_image_ensembles(data_file, 'imcrop', resample=True))\n",
    "\n",
    "    for val, test in zip(val_expts, test_expts):\n",
    "        expt_settings = []\n",
    "        print(val[-1])\n",
    "        for expt_name in val[1]:\n",
    "            resampled_accs = resample_wrapper(val[0], test[0], expt_name, ens_size=16, \n",
    "                                              add_aug=True, aug_name='image_ensemble_imcrop', verbose=False)            \n",
    "            resampled_accs['expt_name'] = expt_name\n",
    "            expt_settings.append(resampled_accs)\n",
    "            print(\"done\")            \n",
    "            \n",
    "        best_expt = max(expt_settings, key=lambda x: x['val_avg']) # take the val accuracy, avged over samples\n",
    "        df['classifier'].append(classifier+'_crop')    \n",
    "        df['acc'].append(best_expt['test_avg'])\n",
    "        df['stderr'].append(best_expt['test_stderr'])\n",
    "        df['expt'].append(best_expt['expt_name'])\n",
    "        df['expt_group'].append(test[2])\n",
    "\n",
    "df = pd.DataFrame.from_dict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plot it\n",
    "f, ax = plt.subplots(1, 1, figsize=(14, 6))\n",
    "\n",
    "group_size = 8\n",
    "bar_width=0.1\n",
    "n_groups = 8\n",
    "bar_offsets = bar_offset(group_size, n_groups, bar_width)\n",
    "palette = make_yellow_palette(3)[1:] + make_blue_palette(3)[1:] + make_green_palette(3)[1:] + make_purple_palette(3)[1:]\n",
    "\n",
    "# resample_stats = get_list_stats(im_crops['resamples'])\n",
    "ind = 0.2\n",
    "# ax.axhline(im_crops['acc_ensembled'], color='k', linestyle=':', label='Original Images')\n",
    "ax.bar(bar_offsets[0], [x['acc_original'] for x in im_crop_data], width=bar_width, color=palette[0], \n",
    "       label='Image Single Crop', edgecolor=(0.5, 0.5, 0.5), capsize=5)\n",
    "ax.bar(bar_offsets[1], [get_list_stats(x['resamples'])['mean'] for x in im_crop_data],\n",
    "       width=bar_width, color=palette[1], yerr=[get_list_stats(x['resamples'])['stderr'] for x in im_crop_data],\n",
    "       label='Image Multi Crop', edgecolor=(0.5, 0.5, 0.5), capsize=5)\n",
    "\n",
    "xticklabels = []\n",
    "for i in range(6):\n",
    "    indices = np.arange(i, n_groups*6, 6)\n",
    "    bar_height = df.iloc[indices]['acc']\n",
    "    bar_err = df.iloc[indices]['stderr']\n",
    "    assert(all([x == df.iloc[indices[0]]['expt_group'] for x in df.iloc[indices]['expt_group']]))\n",
    "    ax.bar(bar_offsets[i+2], bar_height, width=bar_width, color=palette[i+2], yerr=bar_err,\n",
    "           label=df.iloc[indices[0]]['expt_group'], edgecolor=(0.5, 0.5, 0.5), capsize=5)\n",
    "    xticklabels.append(df.iloc[indices[0]]['classifier'].replace('_', '\\n'))\n",
    "\n",
    "ax.set_ylim([94, 100])\n",
    "ax.set_xticks(list(range(1, n_groups+1)))\n",
    "handles,labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles, labels, loc='upper center', ncol=4, prop={'size': 11})\n",
    "# ax.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, -0.2), ncol=4, prop={'size': 11})\n",
    "ax.set_xticklabels(['Original\\nImages', 'GAN\\nRecontructions',\n",
    "                    'Isotropic Fine\\nAugmentations', 'Isotropic Coarse\\nAugmentations',\n",
    "                    'PCA Fine\\nAugmentations', 'PCA Coarse\\nAugmentations', \n",
    "                    'Style-mix Fine\\nAugmentations', 'Style-mix Coarse\\nAugmentations'], fontsize=12)\n",
    "ax.set_xlabel('Classifier training distribution', fontsize=16)\n",
    "ax.set_ylabel('Classification Accuracy', fontsize=16)\n",
    "for tick in ax.yaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(14) \n",
    "ax.set_title('Cars', fontsize=16)\n",
    "\n",
    "f.tight_layout()\n",
    "save(f, 'sm_graph_cars_all_settings.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cat face classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure for main: cat face augmentations (does not use crop)\n",
    "df = defaultdict(list)\n",
    "\n",
    "for i, classifier in enumerate(['imageclassifier', 'latentclassifier', \n",
    "                                'latentclassifier_stylemix_coarse']):\n",
    "    print(classifier)\n",
    "    val_expts = [\n",
    "        # also tried without _tensortransform, it's similar\n",
    "        (f'results/precomputed_evaluations/cat/output/{classifier}_val/gan_ensemble_isotropic_coarse_tensortransform.npz', \n",
    "         ('isotropic_coarse_0.50', 'isotropic_coarse_0.70', 'isotropic_coarse_1.00'), 'Isotropic Coarse'),\n",
    "        (f'results/precomputed_evaluations/cat/output/{classifier}_val/gan_ensemble_isotropic_fine_tensortransform.npz', \n",
    "         ('isotropic_fine_0.10', 'isotropic_fine_0.20', 'isotropic_fine_0.30'), 'Isotropic Fine'),\n",
    "        (f'results/precomputed_evaluations/cat/output/{classifier}_val/gan_ensemble_pca_coarse_tensortransform.npz', \n",
    "         ('pca_coarse_0.50', 'pca_coarse_0.70', 'pca_coarse_1.00'), 'PCA Coarse'),\n",
    "        (f'results/precomputed_evaluations/cat/output/{classifier}_val/gan_ensemble_pca_fine_tensortransform.npz',  \n",
    "         ('pca_fine_0.50', 'pca_fine_0.70', 'pca_fine_1.00'), 'PCA Fine'),\n",
    "        (f'results/precomputed_evaluations/cat/output/{classifier}_val/gan_ensemble_stylemix_coarse_tensortransform.npz', \n",
    "         ('stylemix_coarse',), 'Style-mix Coarse'),\n",
    "#         (f'results/precomputed_evaluations/cat/output/{classifier}_val/gan_ensemble_stylemix_fine_tensortransform.npz', \n",
    "#          ('stylemix_fine',), 'Style-mix Fine'),\n",
    "    ]\n",
    "    test_expts = [(x.replace('_val/', '_test/'), y, z) for x, y, z in val_expts]\n",
    "\n",
    "    for val, test in zip(val_expts, test_expts):\n",
    "        expt_settings = []\n",
    "        print(val[-1])\n",
    "        for expt_name in val[1]:\n",
    "            resampled_accs = resample_wrapper(val[0], test[0], expt_name, ens_size=31, \n",
    "                                              add_aug=False, verbose=False)            \n",
    "            resampled_accs['expt_name'] = expt_name\n",
    "            expt_settings.append(resampled_accs)\n",
    "            print(\"done\")            \n",
    "            \n",
    "        best_expt = max(expt_settings, key=lambda x: x['val_avg']) # take the val accuracy, avged over samples\n",
    "        df['classifier'].append(classifier+'_crop')    \n",
    "        df['acc'].append(best_expt['test_avg'])\n",
    "        df['stderr'].append(best_expt['test_stderr'])\n",
    "        df['expt'].append(best_expt['expt_name'])\n",
    "        df['expt_group'].append(test[2])\n",
    "\n",
    "df = pd.DataFrame.from_dict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot it\n",
    "f, ax = plt.subplots(1, 1, figsize=(7, 5))\n",
    "\n",
    "data_file = f'results/precomputed_evaluations/cat/output/imageclassifier_test/image_ensemble_imcrop.npz'\n",
    "im_s = get_accuracy_from_image_ensembles(data_file, 'imcrop', resample=True)\n",
    "\n",
    "group_size = 5\n",
    "bar_width=0.15\n",
    "n_groups = 3\n",
    "bar_offsets = bar_offset(group_size, n_groups, bar_width)\n",
    "palette = make_blue_palette(3)[1:] + make_green_palette(3)[1:] + make_purple_palette(3)[1:]\n",
    "\n",
    "resample_stats = get_list_stats(im_s['resamples'])\n",
    "ind = 0.2\n",
    "# note: using acc_original here, as it's better\n",
    "ax.axhline(im_s['acc_original'], color='k', linestyle=':', label='Original Images')\n",
    "\n",
    "xticklabels = []\n",
    "for i in range(group_size):\n",
    "    indices = np.arange(i, n_groups*group_size, group_size)\n",
    "    bar_height = df.iloc[indices]['acc']\n",
    "    bar_err = df.iloc[indices]['stderr']\n",
    "    assert(all([x == df.iloc[indices[0]]['expt_group'] for x in df.iloc[indices]['expt_group']]))\n",
    "    ax.bar(bar_offsets[i], bar_height, width=bar_width, color=palette[i], yerr=bar_err,\n",
    "           label=df.iloc[indices[0]]['expt_group'], edgecolor=(0.5, 0.5, 0.5), capsize=5)\n",
    "    xticklabels.append(df.iloc[indices[0]]['classifier'].replace('_', '\\n'))\n",
    "\n",
    "ax.set_ylim([90, 95])\n",
    "ax.set_xticks(list(range(1, n_groups+1)))\n",
    "handles,labels = ax.get_legend_handles_labels()\n",
    "# reorder it so it looks nicer\n",
    "order = [0, 3, 1, 4, 2, 5]\n",
    "handles = [handles[i] for i in order]\n",
    "labels = [labels[i] for i in order]\n",
    "ax.legend(handles, labels, loc='upper center', ncol=3, prop={'size': 10.8})\n",
    "# ax.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, -0.3), ncol=3, prop={'size': 11})\n",
    "# ax.legend(loc='center left', bbox_to_anchor=(1, 0.5), prop={'size': 14})\n",
    "ax.set_xticklabels(['Original\\nImages', 'GAN\\nRecontructions', 'Style-mix Coarse\\nAugmentations'], fontsize=12)\n",
    "ax.set_xlabel('Classifier training distribution', fontsize=16)\n",
    "ax.set_ylabel('Classification Accuracy', fontsize=16)\n",
    "for tick in ax.yaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(14) \n",
    "ax.set_title('Cats', fontsize=16)\n",
    "\n",
    "f.tight_layout()\n",
    "save(f, 'graph_cats_v2.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all settings for the supplemental\n",
    "df = defaultdict(list)\n",
    "im_crop_data = []\n",
    "\n",
    "for i, classifier in enumerate(['imageclassifier', 'latentclassifier', \n",
    "                                'latentclassifier_isotropic_fine', 'latentclassifier_isotropic_coarse',\n",
    "                                'latentclassifier_pca_fine', 'latentclassifier_pca_coarse',\n",
    "                                'latentclassifier_stylemix_fine', 'latentclassifier_stylemix_coarse']):\n",
    "    print(classifier)\n",
    "    val_expts = [\n",
    "        (f'results/precomputed_evaluations/cat/output/{classifier}_val/gan_ensemble_isotropic_coarse_tensortransform.npz', \n",
    "         ('isotropic_coarse_0.50', 'isotropic_coarse_0.70', 'isotropic_coarse_1.00'), 'Isotropic Coarse'),\n",
    "        (f'results/precomputed_evaluations/cat/output/{classifier}_val/gan_ensemble_isotropic_fine_tensortransform.npz', \n",
    "         ('isotropic_fine_0.10', 'isotropic_fine_0.20', 'isotropic_fine_0.30'), 'Isotropic Fine'),\n",
    "        (f'results/precomputed_evaluations/cat/output/{classifier}_val/gan_ensemble_pca_coarse_tensortransform.npz', \n",
    "         ('pca_coarse_0.50', 'pca_coarse_0.70', 'pca_coarse_1.00'), 'PCA Coarse'),\n",
    "        (f'results/precomputed_evaluations/cat/output/{classifier}_val/gan_ensemble_pca_fine_tensortransform.npz',  \n",
    "         ('pca_fine_0.50', 'pca_fine_0.70', 'pca_fine_1.00'), 'PCA Fine'),\n",
    "        (f'results/precomputed_evaluations/cat/output/{classifier}_val/gan_ensemble_stylemix_coarse_tensortransform.npz', \n",
    "         ('stylemix_coarse',), 'Style-mix Coarse'),\n",
    "        (f'results/precomputed_evaluations/cat/output/{classifier}_val/gan_ensemble_stylemix_fine_tensortransform.npz', \n",
    "         ('stylemix_fine',), 'Style-mix Fine'),\n",
    "    ]\n",
    "    test_expts = [(x.replace('_val/', '_test/'), y, z) for x, y, z in val_expts]\n",
    "    data_file = f'results/precomputed_evaluations/cat/output/{classifier}_test/image_ensemble_imcrop.npz'\n",
    "    im_crop_data.append(get_accuracy_from_image_ensembles(data_file, 'imcrop', resample=True))\n",
    "\n",
    "    for val, test in zip(val_expts, test_expts):\n",
    "        expt_settings = []\n",
    "        print(val[-1])\n",
    "        for expt_name in val[1]:\n",
    "            resampled_accs = resample_wrapper(val[0], test[0], expt_name, ens_size=31, \n",
    "                                              add_aug=False, verbose=False)            \n",
    "            resampled_accs['expt_name'] = expt_name\n",
    "            expt_settings.append(resampled_accs)\n",
    "            print(\"done\")            \n",
    "            \n",
    "        best_expt = max(expt_settings, key=lambda x: x['val_avg']) # take the val accuracy, avged over samples\n",
    "        df['classifier'].append(classifier)    \n",
    "        df['acc'].append(best_expt['test_avg'])\n",
    "        df['stderr'].append(best_expt['test_stderr'])\n",
    "        df['expt'].append(best_expt['expt_name'])\n",
    "        df['expt_group'].append(test[2])\n",
    "\n",
    "df = pd.DataFrame.from_dict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot it\n",
    "f, ax = plt.subplots(1, 1, figsize=(14, 6))\n",
    "\n",
    "group_size = 8\n",
    "bar_width=0.1\n",
    "n_groups = 8\n",
    "bar_offsets = bar_offset(group_size, n_groups, bar_width)\n",
    "palette = make_yellow_palette(3)[1:] + make_blue_palette(3)[1:] + make_green_palette(3)[1:] + make_purple_palette(3)[1:]\n",
    "\n",
    "ind = 0.2\n",
    "# ax.axhline(im_crops['acc_ensembled'], color='k', linestyle=':', label='Original Images')\n",
    "ax.bar(bar_offsets[0], [x['acc_original'] for x in im_crop_data], width=bar_width, color=palette[0], \n",
    "       label='Image Single Crop', edgecolor=(0.5, 0.5, 0.5), capsize=5)\n",
    "ax.bar(bar_offsets[1], [get_list_stats(x['resamples'])['mean'] for x in im_crop_data],\n",
    "       width=bar_width, color=palette[1], yerr=[get_list_stats(x['resamples'])['stderr'] for x in im_crop_data],\n",
    "       label='Image Multi Crop', edgecolor=(0.5, 0.5, 0.5), capsize=5)\n",
    "\n",
    "\n",
    "xticklabels = []\n",
    "for i in range(6):\n",
    "    indices = np.arange(i, n_groups*6, 6)\n",
    "    bar_height = df.iloc[indices]['acc']\n",
    "    bar_err = df.iloc[indices]['stderr']\n",
    "    assert(all([x == df.iloc[indices[0]]['expt_group'] for x in df.iloc[indices]['expt_group']]))\n",
    "    ax.bar(bar_offsets[i+2], bar_height, width=bar_width, color=palette[i+2], yerr=bar_err,\n",
    "           label=df.iloc[indices[0]]['expt_group'], edgecolor=(0.5, 0.5, 0.5), capsize=5)\n",
    "    xticklabels.append(df.iloc[indices[0]]['classifier'].replace('_', '\\n'))\n",
    "\n",
    "ax.set_ylim([90, 94])\n",
    "ax.set_xticks(list(range(1, n_groups+1)))\n",
    "handles,labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles, labels, loc='upper center', ncol=4, prop={'size': 11})\n",
    "# ax.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, -0.2), ncol=4, prop={'size': 11})\n",
    "ax.set_xticklabels(['Original\\nImages', 'GAN\\nRecontructions',\n",
    "                    'Isotropic Fine\\nAugmentations', 'Isotropic Coarse\\nAugmentations',\n",
    "                    'PCA Fine\\nAugmentations', 'PCA Coarse\\nAugmentations', \n",
    "                    'Style-mix Fine\\nAugmentations', 'Style-mix Coarse\\nAugmentations'], fontsize=12)\n",
    "ax.set_xlabel('Classifier training distribution', fontsize=16)\n",
    "ax.set_ylabel('Classification Accuracy', fontsize=16)\n",
    "for tick in ax.yaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(14) \n",
    "ax.set_title('Cats', fontsize=16)\n",
    "\n",
    "f.tight_layout()\n",
    "save(f, 'sm_graph_cats_all_settings.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stylegan faces 40 attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_mean = data_celebahq.attr_celebahq.mean(axis=0)[:-1]\n",
    "attr_order = sorted([(abs(v-0.5), v, k) for k, v in attr_mean.to_dict().items()])\n",
    "\n",
    "table_dict = OrderedDict([])\n",
    "table_accs = OrderedDict([])\n",
    "\n",
    "\n",
    "for i, (_, _, attr) in enumerate(tqdm(attr_order[:40])):\n",
    "    # print('========== %s ==========' % attr)\n",
    "    \n",
    "    # gan jitter\n",
    "    val_file = f'results/precomputed_evaluations/celebahq/output/{attr}_val/gan_ensemble_stylemix_fine.npz'\n",
    "    test_file = f'results/precomputed_evaluations/celebahq/output/{attr}_test/gan_ensemble_stylemix_fine.npz'\n",
    "    expt_name = 'stylemix_fine'\n",
    "    # resample\n",
    "    resampled_accs = resample_wrapper(val_file, test_file, expt_name, ens_size=31, \n",
    "                                      add_aug=False, verbose=False)   \n",
    "    val_orig = resampled_accs['val_acc_original']\n",
    "    val_top1 = resampled_accs['val_avg']\n",
    "    test_orig = resampled_accs['test_acc_original']\n",
    "    test_top1_from_val = resampled_accs['test_avg']\n",
    "    \n",
    "\n",
    "    # gan jitter with color/crop jitter\n",
    "    val_file = f'results/precomputed_evaluations/celebahq/output/{attr}_val/gan_ensemble_stylemix_fine_tensortransform.npz'\n",
    "    test_file = f'results/precomputed_evaluations/celebahq/output/{attr}_test/gan_ensemble_stylemix_fine_tensortransform.npz'\n",
    "    expt_name = 'stylemix_fine'\n",
    "    # resample\n",
    "    resampled_accs = resample_wrapper(val_file, test_file, expt_name, ens_size=31, \n",
    "                                      add_aug=False, verbose=False)   \n",
    "    val_orig_mix = resampled_accs['val_acc_original']\n",
    "    val_top1_mix = resampled_accs['val_avg']\n",
    "    test_orig_mix = resampled_accs['test_acc_original']\n",
    "    test_top1_from_val_mix = resampled_accs['test_avg']\n",
    "    \n",
    "    # color jitter\n",
    "    val_file = f'results/precomputed_evaluations/celebahq/output/{attr}_val/image_ensemble_imcolor.npz'\n",
    "    im_ensemble = get_accuracy_from_image_ensembles(val_file, 'imcolor', resample=True, verbose=False)\n",
    "    val_color_orig = im_ensemble['acc_original']\n",
    "    val_color_ens = np.mean(im_ensemble['resamples']) # im_ensemble['acc_ensembled']\n",
    "    test_file = f'results/precomputed_evaluations/celebahq/output/{attr}_test/image_ensemble_imcolor.npz'\n",
    "    im_ensemble = get_accuracy_from_image_ensembles(test_file, 'imcolor', resample=True, verbose=False)\n",
    "    test_color_orig = im_ensemble['acc_original']\n",
    "    test_color_ens = np.mean(im_ensemble['resamples']) # im_ensemble['acc_ensembled']\n",
    "    \n",
    "    # crop jitter\n",
    "    val_file = f'results/precomputed_evaluations/celebahq/output/{attr}_val/image_ensemble_imcrop.npz'\n",
    "    im_ensemble = get_accuracy_from_image_ensembles(val_file, 'imcrop', resample=True, verbose=False)\n",
    "    val_crop_orig = im_ensemble['acc_original']\n",
    "    val_crop_ens = np.mean(im_ensemble['resamples']) # im_ensemble['acc_ensembled']\n",
    "    test_file = f'results/precomputed_evaluations/celebahq/output/{attr}_test/image_ensemble_imcrop.npz'\n",
    "    im_ensemble = get_accuracy_from_image_ensembles(test_file, 'imcrop', resample=True, verbose=False)\n",
    "    test_crop_orig = im_ensemble['acc_original']\n",
    "    test_crop_ens = np.mean(im_ensemble['resamples']) # im_ensemble['acc_ensembled']\n",
    "    \n",
    "    # sanity check\n",
    "    assert(test_color_orig == test_orig)\n",
    "    assert(test_crop_orig == test_orig)\n",
    "    assert(test_orig_mix == test_orig)\n",
    "    assert(val_color_orig == val_orig)\n",
    "    assert(val_crop_orig == val_orig)\n",
    "    assert(val_orig_mix == val_orig)\n",
    "    \n",
    "    val_labels = ['Val Orig', 'Val Color', 'Val Crop', 'Val GAN', 'Val Combined']\n",
    "    val_values = [val_orig, val_color_ens, val_crop_ens, val_top1, val_top1_mix]\n",
    "    val_diffs = [x - val_values[0] for x in val_values]\n",
    "    \n",
    "    test_labels = ['Test Orig', 'Test Color', 'Test Crop', 'Test GAN', 'Test Combined']\n",
    "    test_values = [test_orig, test_color_ens, test_crop_ens, test_top1_from_val, test_top1_from_val_mix]\n",
    "    test_diffs = [x - test_values[0] for x in test_values]\n",
    "    table_dict[attr] = val_diffs + test_diffs\n",
    "    table_accs[attr] = val_values + test_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.DataFrame.from_dict(table_dict, orient='index', columns=val_labels+test_labels)\n",
    "table = table.append(table.mean(axis=0).rename('Avg'))\n",
    "std = table.iloc[:-1, :].std(axis=0).rename('Std')\n",
    "print(std / np.sqrt(40))\n",
    "display(table.iloc[-1:, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_acc = pd.DataFrame.from_dict(table_accs, orient='index', columns=val_labels+test_labels)\n",
    "table_acc = table_acc.append(table_acc.mean(axis=0).rename('Avg'))\n",
    "std_acc = table_acc.iloc[:-1, :].std(axis=0).rename('Std')\n",
    "print(std_acc / np.sqrt(40))\n",
    "display(table_acc.iloc[-1:, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = table_acc.iloc[[-1], 5:].T\n",
    "df = df.reset_index()\n",
    "display(df)\n",
    "f, ax = plt.subplots(1, 1, figsize=(6, 3))\n",
    "palette = adjust_saturation(make_blue_palette(3), 0.3)\n",
    "ax.bar(np.arange(len(df)), df.loc[:, 'Avg'], color=palette[-1], edgecolor=(0.5, 0.5, 0.5))\n",
    "ax.set_ylim([88.5, 89.5])\n",
    "ax.set_xticks(range(5))\n",
    "ax.set_xticklabels(['Single\\nImage', 'Color\\nJitter', 'Crop\\nJitter', 'Style-mix\\nJitter', 'Combined\\nJitter'], \n",
    "                   fontsize=12)\n",
    "ax.set_ylabel('Classification Accuracy', fontsize=16)\n",
    "for tick in ax.yaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(12) \n",
    "ax.set_xlabel('')\n",
    "ax.set_xlim([-0.7, 4.7])\n",
    "save(f, 'graph_face_testaug.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 1, figsize=(6, 3))\n",
    "\n",
    "diffs = table.iloc[:-1, 5:]\n",
    "bar_height = diffs.mean(axis=0)\n",
    "bar_err = diffs.std(axis=0) / np.sqrt(diffs.shape[0])\n",
    "palette = adjust_saturation(make_blue_palette(3), 0.3)\n",
    "ax.bar(range(5), bar_height, edgecolor=(0.5, 0.5, 0.5), yerr=bar_err, color=palette[-1], capsize=5)\n",
    "ax.set_xticks(range(5))\n",
    "ax.set_xticklabels(['Single\\nImage', 'Color\\nJitter', 'Crop\\nJitter', 'Style-mix\\nJitter', 'Combined\\nJitter'], \n",
    "                   fontsize=12)\n",
    "ax.set_ylabel('Accuracy Difference', fontsize=16)\n",
    "for tick in ax.yaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(12) \n",
    "ax.set_xlabel('')\n",
    "ax.set_xlim([-0.7, 4.7])\n",
    "ax.set_ylim([-0.1, 0.2])\n",
    "save(f, 'graph_face_testaug_diffs.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stylegan idinvert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_mean = data_celebahq.attr_celebahq.mean(axis=0)[:-1]\n",
    "attr_order = sorted([(abs(v-0.5), v, k) for k, v in attr_mean.to_dict().items()])\n",
    "\n",
    "table_dict = OrderedDict([])\n",
    "table_accs = OrderedDict([])\n",
    "\n",
    "for i, (_, _, attr) in enumerate(tqdm(attr_order[:40])):\n",
    "    # print('========== %s ==========' % attr)\n",
    "    \n",
    "    # gan jitter\n",
    "    val_file = f'results/precomputed_evaluations/celebahq-idinvert/output/{attr}_val/gan_ensemble_stylemix_fine.npz'\n",
    "    test_file = f'results/precomputed_evaluations/celebahq-idinvert/output/{attr}_test/gan_ensemble_stylemix_fine.npz'\n",
    "    expt_name = 'stylemix_fine'\n",
    "    # resample\n",
    "    resampled_accs = resample_wrapper(val_file, test_file, expt_name, ens_size=31, \n",
    "                                      add_aug=False, verbose=False)   \n",
    "    val_orig = resampled_accs['val_acc_original']\n",
    "    val_top1 = resampled_accs['val_avg']\n",
    "    test_orig = resampled_accs['test_acc_original']\n",
    "    test_top1_from_val = resampled_accs['test_avg']\n",
    "\n",
    "    # gan jitter with color/crop jitter\n",
    "    val_file = f'results/precomputed_evaluations/celebahq-idinvert/output/{attr}_val/gan_ensemble_stylemix_fine_tensortransform.npz'\n",
    "    test_file = f'results/precomputed_evaluations/celebahq-idinvert/output/{attr}_test/gan_ensemble_stylemix_fine_tensortransform.npz'\n",
    "    expt_name = 'stylemix_fine'\n",
    "    # resample\n",
    "    resampled_accs = resample_wrapper(val_file, test_file, expt_name, ens_size=31, \n",
    "                                      add_aug=False, verbose=False)   \n",
    "    val_orig_mix = resampled_accs['val_acc_original']\n",
    "    val_top1_mix = resampled_accs['val_avg']\n",
    "    test_orig_mix = resampled_accs['test_acc_original']\n",
    "    test_top1_from_val_mix = resampled_accs['test_avg']\n",
    "    \n",
    "    # sanity check\n",
    "    assert(test_orig_mix == test_orig)\n",
    "    assert(val_orig_mix == val_orig)\n",
    "    \n",
    "    val_labels = ['Val Orig', 'Val GAN', 'Val Combined']\n",
    "    val_values = [val_orig, val_top1, val_top1_mix]\n",
    "    val_diffs = [x - val_values[0] for x in val_values]\n",
    "    \n",
    "    test_labels = ['Test Orig',  'Test GAN', 'Test Combined']\n",
    "    test_values = [test_orig,test_top1_from_val, test_top1_from_val_mix]\n",
    "    test_diffs = [x - test_values[0] for x in test_values]\n",
    "    table_dict[attr] = val_diffs + test_diffs\n",
    "    table_accs[attr] = val_values + test_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_idinvert = pd.DataFrame.from_dict(table_dict, orient='index', columns=val_labels+test_labels)\n",
    "table_idinvert = table_idinvert.append(table_idinvert.mean(axis=0).rename('Avg'))\n",
    "std = table_idinvert.iloc[:-1, :].std(axis=0).rename('Std')\n",
    "print(std / np.sqrt(40))\n",
    "\n",
    "display(table_idinvert.iloc[-1:, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_idinvert_acc = pd.DataFrame.from_dict(table_accs, orient='index', columns=val_labels+test_labels)\n",
    "table_idinvert_acc = table_idinvert_acc.append(table_idinvert_acc.mean(axis=0).rename('Avg'))\n",
    "std_acc = table_idinvert_acc.iloc[:-1, :].std(axis=0).rename('Std')\n",
    "print(std_acc / np.sqrt(40))\n",
    "display(table_idinvert_acc.iloc[-1:, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "ax.plot(table['Test GAN'], table_idinvert['Test GAN'], '*', label='GAN Aug')\n",
    "ax.plot(table['Test Combined'], table_idinvert['Test Combined'], '*', label='Combined Aug')\n",
    "ax.set_xlabel('Pre-trained FFHQ + Encoder\\nAccuracy Difference', fontsize=14)\n",
    "ax.set_ylabel('ID-Invert\\nAccuracy Difference', fontsize=14)\n",
    "ax.legend(loc='lower right')\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "corr, pval = pearsonr(table['Test GAN'].to_list() + table['Test Combined'].to_list(), \n",
    "                   table_idinvert['Test GAN'].to_list() + table_idinvert['Test Combined'].to_list())\n",
    "print('Pearsons correlation: %.3f pval %f' % (corr, pval))\n",
    "save(f, 'sm_graph_face_idinvert.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# different training approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different training approaches\n",
    "attr_mean = data_celebahq.attr_celebahq.mean(axis=0)[:-1]\n",
    "attr_order = sorted([(abs(v-0.5), v, k) for k, v in attr_mean.to_dict().items()])\n",
    "\n",
    "table_dict = OrderedDict([])\n",
    "table_accs = OrderedDict([])\n",
    "\n",
    "for i, (_, _, attribute) in enumerate(tqdm(attr_order)):\n",
    "    \n",
    "    val_values = []\n",
    "    val_diffs = []\n",
    "    test_values = []\n",
    "    test_diffs = []\n",
    "    val_labels = ['Val ' + train_method + ' ' + eval_method for train_method in \n",
    "                  ['Im', 'latent', 'latent_stylemix', 'latent_stylemix_crop'] for eval_method in ['Single', 'GAN Ens', 'Combined Ens']]\n",
    "    test_labels = ['Test ' + train_method + ' ' + eval_method for train_method in \n",
    "                   ['Im', 'latent', 'latent_stylemix', 'latent_stylemix_crop'] for eval_method in ['Single', 'GAN Ens', 'Combined Ens']]\n",
    "    for suffix in ['', '__latent', '__latent_stylemix_fine', '__latent_stylemix_fine_crop']:\n",
    "        attr = attribute + suffix\n",
    "        # print('========== %s ==========' % attr)\n",
    "\n",
    "        # gan jitter\n",
    "        val_file = f'results/precomputed_evaluations/celebahq/output/{attr}_val/gan_ensemble_stylemix_fine.npz'\n",
    "        test_file = f'results/precomputed_evaluations/celebahq/output/{attr}_test/gan_ensemble_stylemix_fine.npz'\n",
    "        expt_name = 'stylemix_fine'\n",
    "        # resample\n",
    "        resampled_accs = resample_wrapper(val_file, test_file, expt_name, ens_size=31, \n",
    "                                          add_aug=False, verbose=False)   \n",
    "        val_orig = resampled_accs['val_acc_original']\n",
    "        val_top1 = resampled_accs['val_avg']\n",
    "        test_orig = resampled_accs['test_acc_original']\n",
    "        test_top1_from_val = resampled_accs['test_avg']\n",
    "\n",
    "        # gan jitter with color/crop jitter\n",
    "        val_file = f'results/precomputed_evaluations/celebahq/output/{attr}_val/gan_ensemble_stylemix_fine_tensortransform.npz'\n",
    "        test_file = f'results/precomputed_evaluations/celebahq/output/{attr}_test/gan_ensemble_stylemix_fine_tensortransform.npz'\n",
    "        expt_name = 'stylemix_fine'\n",
    "        # resample\n",
    "        resampled_accs = resample_wrapper(val_file, test_file, expt_name, ens_size=31, \n",
    "                                          add_aug=False, verbose=False)   \n",
    "        val_orig_mix = resampled_accs['val_acc_original']\n",
    "        val_top1_mix = resampled_accs['val_avg']\n",
    "        test_orig_mix = resampled_accs['test_acc_original']\n",
    "        test_top1_from_val_mix = resampled_accs['test_avg']\n",
    "    \n",
    "        # sanity check\n",
    "        assert(test_orig_mix == test_orig)\n",
    "        assert(val_orig_mix == val_orig)\n",
    "\n",
    "        new_val_values = [val_orig, val_top1, val_top1_mix]\n",
    "        new_test_values = [test_orig, test_top1_from_val, test_top1_from_val_mix]\n",
    "        val_values.extend(new_val_values)\n",
    "        test_values.extend(new_test_values)\n",
    "        val_diffs.extend([x - val_values[0] for x in new_val_values])\n",
    "        test_diffs.extend([x - test_values[0] for x in new_test_values])\n",
    "        \n",
    "    table_dict[attribute] = val_diffs + test_diffs\n",
    "    table_accs[attribute] = val_values + test_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.DataFrame.from_dict(table_dict, orient='index', columns=val_labels+test_labels)\n",
    "table = table.append(table.mean(axis=0).rename('Avg'))\n",
    "std = table.iloc[:-1, :].std(axis=0).rename('Std')\n",
    "print(std / np.sqrt(40))\n",
    "# table = table.append(table.iloc[:-1, :].std(axis=0).rename('Std'))\n",
    "# display(table.iloc[-2:, 12:])\n",
    "\n",
    "display(table.iloc[-1:, 12:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_acc = pd.DataFrame.from_dict(table_accs, orient='index', columns=val_labels+test_labels)\n",
    "table_acc = table_acc.append(table_acc.mean(axis=0).rename('Avg'))\n",
    "# table_acc.iloc[:, 12:]\n",
    "display(table_acc.iloc[-1:, 12:])\n",
    "# show the IM and W columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(table_acc.iloc[:-1, 12:].shape[0] == 40)\n",
    "df = {'train_method': ['Im', 'Im', 'Im', 'latent', 'latent', 'latent'] + ['latent_stylemix'] * 3 + ['latent_stylemix_crop'] * 3,\n",
    "      'ens_method': ['Single Image', 'Style-mix Ensemble', 'Combined Ensemble'] * 4,\n",
    "      'acc': table_acc.iloc[:-1, 12:].mean(axis=0),\n",
    "      'stderr': table_acc.iloc[:-1, 12:].std(axis=0) / np.sqrt(table_acc.iloc[:-1, 12:].shape[0])\n",
    "     }\n",
    "df = pd.DataFrame.from_dict(df)\n",
    "display(df)\n",
    "\n",
    "f, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "group_size = 3\n",
    "bar_width=0.2\n",
    "n_groups = 4\n",
    "bar_offsets = bar_offset(group_size, n_groups, bar_width)\n",
    "palette = make_blue_palette(group_size)\n",
    "\n",
    "xticklabels = []\n",
    "for i in range(group_size):\n",
    "    indices = np.arange(i, n_groups*group_size, group_size)\n",
    "    bar_height = df.iloc[indices]['acc']\n",
    "    bar_err = df.iloc[indices]['stderr']\n",
    "    assert(all([x == df.iloc[indices[0]]['ens_method'] for x in df.iloc[indices]['ens_method']]))\n",
    "    ax.bar(bar_offsets[i], bar_height, width=bar_width, color=palette[i], \n",
    "           label=df.iloc[indices[0]]['ens_method'], edgecolor=(0.5, 0.5, 0.5), capsize=5)\n",
    "    xticklabels.append(df.iloc[indices[0]]['train_method'].replace('_', '\\n'))\n",
    "ax.set_ylim([88.5, 89.8])\n",
    "ax.legend(prop={'size': 12}) # , loc='upper left')\n",
    "# ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1), ncol=2, prop={'size': 11})\n",
    "ax.set_xticks(np.arange(1,n_groups+1))\n",
    "ax.set_xticklabels(['Train\\nImage', 'Train\\nLatent', 'Train\\nStyle-mix', 'Train\\nCombined'], fontsize=14)\n",
    "for tick in ax.yaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(12) \n",
    "ax.set_xlabel('')\n",
    "ax.set_ylabel('Accuracy', fontsize=16)\n",
    "f.tight_layout()\n",
    "save(f, 'graph_face_train_latent.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(table.iloc[:-1, 12:].shape[0] == 40)\n",
    "df = {'train_method': ['Im', 'Im', 'Im', 'latent', 'latent', 'latent'] + ['latent_stylemix'] * 3 + ['latent_stylemix_crop'] * 3,\n",
    "      'ens_method': ['Single Image', 'Style-mix Ensemble', 'Combined Ensemble'] * 4,\n",
    "      'acc': table.iloc[:-1, 12:].mean(axis=0),\n",
    "      'stderr': table.iloc[:-1, 12:].std(axis=0) / np.sqrt(table.iloc[:-1, 12:].shape[0])\n",
    "     }\n",
    "df = pd.DataFrame.from_dict(df)\n",
    "display(df)\n",
    "\n",
    "f, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "group_size = 3\n",
    "bar_width=0.2\n",
    "n_groups = 4\n",
    "bar_offsets = bar_offset(group_size, n_groups, bar_width)\n",
    "palette = make_blue_palette(group_size)\n",
    "\n",
    "xticklabels = []\n",
    "for i in range(group_size):\n",
    "    indices = np.arange(i, n_groups*group_size, group_size)\n",
    "    bar_height = df.iloc[indices]['acc']\n",
    "    bar_err = df.iloc[indices]['stderr']\n",
    "    assert(all([x == df.iloc[indices[0]]['ens_method'] for x in df.iloc[indices]['ens_method']]))\n",
    "    ax.bar(bar_offsets[i], bar_height, width=bar_width, color=palette[i],  yerr=bar_err,\n",
    "           label=df.iloc[indices[0]]['ens_method'], edgecolor=(0.5, 0.5, 0.5), capsize=5)\n",
    "    xticklabels.append(df.iloc[indices[0]]['train_method'].replace('_', '\\n'))\n",
    "# ax.set_ylim([88.5, 89.6])\n",
    "ax.set_ylim([-0.3, 0.8])\n",
    "ax.legend(prop={'size': 12}, loc='upper left')\n",
    "# ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1), ncol=2, prop={'size': 11})\n",
    "ax.set_xticks(np.arange(1,n_groups+1))\n",
    "ax.set_xticklabels(['Train\\nImage', 'Train\\nLatent', 'Train\\nStyle-mix', 'Train\\nCombined'], fontsize=14)\n",
    "for tick in ax.yaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(12) \n",
    "ax.set_xlabel('')\n",
    "ax.set_ylabel('Accuracy Difference', fontsize=16)\n",
    "f.tight_layout()\n",
    "save(f, 'graph_face_train_latent_diff.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# distribution of classification accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "ax.hist(table_acc['Test Im Single'])\n",
    "ax.set_xlim([50, 100])\n",
    "ax.set_ylabel('Count', fontsize=14)\n",
    "ax.set_xlabel('Test Accuracy', fontsize=14)\n",
    "for tick in ax.yaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(12) \n",
    "for tick in ax.xaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(12) \n",
    "save(f, 'sm_graph_face_acc_distribution.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# over 12 attributes, plot stylemix, isotropic, and PCA fine and coarse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_mean = data_celebahq.attr_celebahq.mean(axis=0)[:-1]\n",
    "attr_order = sorted([(abs(v-0.5), v, k) for k, v in attr_mean.to_dict().items()])\n",
    "\n",
    "df_val = defaultdict(list)\n",
    "df_test = defaultdict(list)\n",
    "\n",
    "for i, (_, _, attr) in enumerate(tqdm(attr_order[:12])):\n",
    "    # print('========== %s ==========' % attr)\n",
    "    \n",
    "    val_expts = [\n",
    "        (f'results/precomputed_evaluations/celebahq/output/{attr}_val/gan_ensemble_isotropic_coarse.npz', \n",
    "         ('isotropic_coarse_0.10', 'isotropic_coarse_0.30'), 'Isotropic Coarse'),\n",
    "        (f'results/precomputed_evaluations/celebahq/output/{attr}_val/gan_ensemble_isotropic_fine.npz', \n",
    "         ('isotropic_fine_0.10', 'isotropic_fine_0.30'), 'Isotropic Fine'),\n",
    "        (f'results/precomputed_evaluations/celebahq/output/{attr}_val/gan_ensemble_pca_coarse.npz', \n",
    "         ('pca_coarse_1.00', 'pca_coarse_2.00', 'pca_coarse_3.00'), 'PCA Coarse'),\n",
    "        (f'results/precomputed_evaluations/celebahq/output/{attr}_val/gan_ensemble_pca_fine.npz',\n",
    "         ('pca_fine_1.00', 'pca_fine_2.00', 'pca_fine_3.00'), 'PCA Fine'),\n",
    "        (f'results/precomputed_evaluations/celebahq/output/{attr}_val/gan_ensemble_stylemix_coarse.npz', \n",
    "         ('stylemix_coarse',), 'Style-mix Coarse'),\n",
    "        (f'results/precomputed_evaluations/celebahq/output/{attr}_val/gan_ensemble_stylemix_fine.npz', \n",
    "         ('stylemix_fine',), 'Style-mix Fine'),\n",
    "    ]\n",
    "    test_expts = [(x.replace('_val/', '_test/'), y, z) for x, y, z in val_expts]\n",
    "    for i, (val, test) in enumerate(zip(val_expts, test_expts)):\n",
    "        expt_settings = []\n",
    "        for expt_name in val[1]:\n",
    "            resampled_accs = resample_wrapper(val[0], test[0], expt_name, ens_size=31, \n",
    "                                              add_aug=False, verbose=False)            \n",
    "            resampled_accs['expt_name'] = expt_name\n",
    "            expt_settings.append(resampled_accs)\n",
    "            \n",
    "        # these should all be the same -- just standard test info\n",
    "        assert(all([x['val_acc_original'] == expt_settings[0]['val_acc_original'] for x in expt_settings]))\n",
    "        assert(all([x['test_acc_original'] == expt_settings[0]['test_acc_original'] for x in expt_settings]))\n",
    "        \n",
    "        if i == 0:\n",
    "            df_val['attribute'].append(attr)\n",
    "            df_val['acc'].append(expt_settings[0]['val_acc_original'])\n",
    "            df_val['stderr'].append(0.)\n",
    "            df_val['expt_group'].append('Original Image')\n",
    "            df_val['expt'].append('original')\n",
    "            df_test['attribute'].append(attr)\n",
    "            df_test['acc'].append(expt_settings[0]['test_acc_original'])\n",
    "            df_test['stderr'].append(0.)\n",
    "            df_test['expt_group'].append('Original Image')\n",
    "            df_test['expt'].append('original')\n",
    "\n",
    "        \n",
    "        # import pdb; pdb.set_trace()\n",
    "        best_expt = max(expt_settings, key=lambda x: x['val_avg']) # take the val accuracy\n",
    "        # val result\n",
    "        df_val['attribute'].append(attr)\n",
    "        df_val['acc'].append(best_expt['val_avg'])\n",
    "        df_val['stderr'].append(best_expt['val_stderr'])\n",
    "        df_val['expt'].append(best_expt['expt_name'])\n",
    "        df_val['expt_group'].append(val[2])\n",
    "        \n",
    "        # test result\n",
    "        df_test['attribute'].append(attr) \n",
    "        df_test['acc'].append(best_expt['test_avg'])\n",
    "        df_test['stderr'].append(best_expt['test_stderr'])\n",
    "        df_test['expt'].append(best_expt['expt_name'])\n",
    "        df_test['expt_group'].append(test[2])\n",
    "        \n",
    "df_val = pd.DataFrame.from_dict(df_val)\n",
    "df_test = pd.DataFrame.from_dict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_per_attr_val = OrderedDict([])\n",
    "group_size=7\n",
    "num_attr=12\n",
    "for i in range(0, num_attr*group_size, group_size):\n",
    "    attribute_names = list(df_val.iloc[i:i+group_size]['attribute'])\n",
    "    assert(all([x == attribute_names[0] for x in attribute_names]))\n",
    "    df_per_attr_val[attribute_names[0]] = list(df_val.iloc[i:i+group_size]['acc'])\n",
    "df_per_attr_val = pd.DataFrame.from_dict(df_per_attr_val, orient='index', columns=['Original'] + [x for _,_, x in val_expts])\n",
    "\n",
    "df_per_attr_test = OrderedDict([])\n",
    "group_size=7\n",
    "num_attr=12\n",
    "for i in range(0, num_attr*group_size, group_size):\n",
    "    attribute_names = list(df_test.iloc[i:i+group_size]['attribute'])\n",
    "    assert(all([x == attribute_names[0] for x in attribute_names]))\n",
    "    df_per_attr_test[attribute_names[0]] = list(df_test.iloc[i:i+group_size]['acc'])\n",
    "df_per_attr_test = pd.DataFrame.from_dict(df_per_attr_test, orient='index', columns=['Original'] + [x for _,_, x in test_expts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_per_attr_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_per_attr_val_diff = (df_per_attr_val.sub(df_per_attr_val['Original'], axis=0)).iloc[:, 1:]\n",
    "df_per_attr_test_diff = (df_per_attr_test.sub(df_per_attr_test['Original'], axis=0)).iloc[:, 1:]\n",
    "f, ax = plt.subplots(1, 1, figsize=(6, 3))\n",
    "\n",
    "group_size = 2\n",
    "bar_width=0.25\n",
    "n_groups = 6\n",
    "bar_offsets = bar_offset(group_size, n_groups, bar_width)\n",
    "palette = sns.color_palette()\n",
    "\n",
    "#### combined plot ####\n",
    "for i, label in enumerate(df_per_attr_val_diff.columns):\n",
    "    \n",
    "    # val\n",
    "    height = df_per_attr_val_diff[label].mean()\n",
    "    yerr = df_per_attr_val_diff[label].std() / np.sqrt(df_per_attr_val_diff.shape[0])\n",
    "    ax.bar(bar_offsets[0][i], height, yerr=yerr, width=bar_width, color=palette[0],\n",
    "           edgecolor=(0.5, 0.5, 0.5), capsize=5, label='Validation' if i == 0 else None)\n",
    "    # test\n",
    "    height = df_per_attr_test_diff[label].mean()\n",
    "    yerr = df_per_attr_test_diff[label].std() / np.sqrt(df_per_attr_test_diff.shape[0])\n",
    "    ax.bar(bar_offsets[1][i], height, yerr=yerr, width=bar_width, color=palette[1],\n",
    "           edgecolor=(0.5, 0.5, 0.5), capsize=5, label='Test' if i == 0 else None)\n",
    "ax.legend()\n",
    "ax.set_ylabel('Accuracy Difference', fontsize=14)\n",
    "ax.set_xticks(np.arange(1,n_groups+1))\n",
    "ax.set_xticklabels([x.replace(' ', '\\n') for x in df_per_attr_val_diff.columns], fontsize=11)\n",
    "save(f, 'graph_face_gan_aug_types.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot the accuracy vs alpha graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for attr in ['Smiling',]:\n",
    "    val_expt = (f'results/precomputed_evaluations/celebahq/output/{attr}_val/gan_ensemble_stylemix_fine_tensortransform.npz', \n",
    "                ('stylemix_fine',), 'Style-Mix Fine')\n",
    "    x, y, z = val_expt\n",
    "    test_expt = (x.replace('_val', '_test'), y, z)\n",
    "\n",
    "    val_res = get_accuracy_from_npz(val_expt[0], val_expt[1][0], add_aug=False, ens_size=31)\n",
    "    test_res = get_accuracy_from_npz(test_expt[0], test_expt[1][0], add_aug=False, ens_size=31)\n",
    "\n",
    "    f, ax = plt.subplots(1, 1, figsize=(6, 3)) # , sharey=True)\n",
    "\n",
    "    ax.plot(val_res['ensemble_table']['weight'], val_res['ensemble_table']['acc'], label='Validation')\n",
    "    ax.plot(test_res['ensemble_table']['weight'], test_res['ensemble_table']['acc'], label='Test')\n",
    "    \n",
    "    # plot the ensemble weight\n",
    "    val_ensemble_table = val_res['ensemble_table']\n",
    "    best_val_setting = val_ensemble_table.iloc[val_ensemble_table['acc'].argsort().iloc[-1], :]\n",
    "    ax.axvline(best_val_setting.weight, color='k', linestyle=':', label='Selected Weight')\n",
    "\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_xlabel('Ensemble Weight')\n",
    "    #for tick in ax.yaxis.get_major_ticks():\n",
    "    #    tick.label.set_fontsize(12) \n",
    "    #for tick in ax.xaxis.get_major_ticks():\n",
    "    #    tick.label.set_fontsize(12) \n",
    "    if attr == 'Smiling':\n",
    "        ax.legend()\n",
    "\n",
    "    # ax.set_title('Attribute: ' + attr.replace('_', ' '), fontsize=16)\n",
    "    # ax[1].set_title('Test', fontsize=16)\n",
    "    # f.suptitle('Attribute: ' + attr.replace('_', ' '), fontsize=16, y=1.0)\n",
    "    f.tight_layout()\n",
    "    save(f, 'sm_ensemble_alpha_%s_v2.pdf' % attr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stylegan corruptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample each 20 times\n",
    "table_dict = OrderedDict([])\n",
    "table_accs = OrderedDict([])\n",
    "table_stderrs = OrderedDict([])\n",
    "\n",
    "# axes = [col for row in axes for col in row]\n",
    "\n",
    "n_samples = 20\n",
    "\n",
    "for i, attribute in enumerate(['Smiling', 'Arched_Eyebrows', 'Young', 'Wavy_Hair']):\n",
    "    val_values = []\n",
    "    test_values = []\n",
    "    val_stderrs = []\n",
    "    test_stderrs = []\n",
    "    val_diffs = []\n",
    "    test_diffs = []\n",
    "    \n",
    "    val_labels = ['Val ' + corruption + ' ' + eval_method for corruption in \n",
    "                  ['Im', 'Jpeg', 'Blur', 'Noise', 'FGSM', 'PGD', 'CW'] for eval_method in ['S', 'R', 'G', 'C']]\n",
    "    test_labels = ['Test ' + corruption + ' ' + eval_method for corruption in \n",
    "                   ['Im', 'Jpeg', 'Blur', 'Noise', 'FGSM', 'PGD', 'CW'] for eval_method in ['S', 'R', 'G', 'C']]    \n",
    "        \n",
    "    for prefix in ['', 'corruption_jpeg_', 'corruption_gaussian_blur_', 'corruption_gaussian_noise_', 'fgsm_', 'pgd_', 'cw_']:\n",
    "        \n",
    "        attr = prefix + attribute\n",
    "\n",
    "        print(attr)\n",
    "        \n",
    "        # gan jitter fine\n",
    "        val_file = f'results/precomputed_evaluations/celebahq/output/{attr}_val/gan_ensemble_stylemix_fine.npz'\n",
    "        test_file = f'results/precomputed_evaluations/celebahq/output/{attr}_test/gan_ensemble_stylemix_fine.npz'\n",
    "        expt_name = 'stylemix_fine'\n",
    "        # resample\n",
    "        resampled_accs = resample_wrapper(val_file, test_file, expt_name, ens_size=31, \n",
    "                                          add_aug=False, verbose=False)   \n",
    "        val_orig = resampled_accs['val_acc_original']\n",
    "        val_top1 = resampled_accs['val_avg']\n",
    "        val_stderr = resampled_accs['val_stderr']\n",
    "        val_rec = resampled_accs['val_acc_rec']\n",
    "        test_orig = resampled_accs['test_acc_original']\n",
    "        test_top1_from_val = resampled_accs['test_avg']\n",
    "        test_stderr = resampled_accs['test_stderr']\n",
    "        test_rec = resampled_accs['test_acc_rec']\n",
    "\n",
    "        # gan jitter with color/crop jitter\n",
    "        val_file = f'results/precomputed_evaluations/celebahq/output/{attr}_val/gan_ensemble_stylemix_fine_tensortransform.npz'\n",
    "        test_file = f'results/precomputed_evaluations/celebahq/output/{attr}_test/gan_ensemble_stylemix_fine_tensortransform.npz'\n",
    "        expt_name = 'stylemix_fine'\n",
    "        resampled_accs = resample_wrapper(val_file, test_file, expt_name, ens_size=31, \n",
    "                                          add_aug=False, verbose=False)   \n",
    "        val_orig_mix = resampled_accs['val_acc_original']\n",
    "        val_top1_mix = resampled_accs['val_avg']\n",
    "        val_stderr_mix = resampled_accs['val_stderr']\n",
    "        val_rec_mix = resampled_accs['val_acc_rec']\n",
    "        test_orig_mix = resampled_accs['test_acc_original']\n",
    "        test_top1_from_val_mix = resampled_accs['test_avg']\n",
    "        test_stderr_mix = resampled_accs['test_stderr']\n",
    "        test_rec_mix = resampled_accs['test_acc_rec']\n",
    "\n",
    "\n",
    "        # sanity check\n",
    "        assert(test_orig_mix == test_orig)\n",
    "        assert(test_rec_mix == test_rec)\n",
    "        assert(val_orig_mix == val_orig)\n",
    "        assert(val_rec_mix == val_rec)\n",
    "\n",
    "\n",
    "        new_val_values = [val_orig, val_rec, val_top1, val_top1_mix]\n",
    "        new_val_stderrs = [0., 0., val_stderr, val_stderr_mix]\n",
    "        new_test_values = [test_orig, test_rec, test_top1_from_val, test_top1_from_val_mix]\n",
    "        new_test_stderrs = [0., 0., test_stderr, test_stderr_mix]\n",
    "\n",
    "        val_values.extend(new_val_values)\n",
    "        test_values.extend(new_test_values)\n",
    "        val_stderrs.extend(new_val_stderrs)\n",
    "        test_stderrs.extend(new_test_stderrs) \n",
    "        val_diffs.extend([x - val_values[0] for x in new_val_values])\n",
    "        test_diffs.extend([x - test_values[0] for x in new_test_values])\n",
    "\n",
    "    table_dict[attribute] = val_diffs + test_diffs\n",
    "    table_accs[attribute] = val_values + test_values\n",
    "    table_stderrs[attribute] = val_stderrs + test_stderrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.DataFrame.from_dict(table_dict, orient='index', columns=val_labels+test_labels)\n",
    "table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(table.iloc[:, 28:])\n",
    "\n",
    "table_acc = pd.DataFrame.from_dict(table_accs, orient='index', columns=val_labels+test_labels)\n",
    "display(table_acc.iloc[:, 28:])\n",
    "\n",
    "table_stderr = pd.DataFrame.from_dict(table_stderrs, orient='index', columns=val_labels+test_labels)\n",
    "display(table_stderr.iloc[:, 28:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(1, 4, figsize=(16, 3.5))\n",
    "\n",
    "for row, attr in enumerate(table_acc.index):\n",
    "    ax = axes[row]\n",
    "    df = {'train_method': ['Uncorrupted'] * 4 + ['Jpeg'] * 4 + ['Blur'] * 4 + ['Noise'] * 4,\n",
    "          'ens_method': ['Image', 'Reconstruction', 'Style-mix Ensemble', 'Combined Ensemble'] * 4,\n",
    "          'acc': table_acc.iloc[row, 28:-12],\n",
    "          'stderr': table_stderr.iloc[row, 28:-12]\n",
    "         }\n",
    "    df = pd.DataFrame.from_dict(df)\n",
    "    # display(df)\n",
    "    palette = make_blue_palette(4)\n",
    "    \n",
    "    group_size=4\n",
    "    n_groups=4\n",
    "    bar_width=0.2\n",
    "    bar_offsets = bar_offset(group_size, n_groups, bar_width)\n",
    "    xticklabels = []\n",
    "    for i in range(group_size):\n",
    "        indices = np.arange(i, n_groups*group_size, group_size)\n",
    "        bar_height = df.iloc[indices]['acc']\n",
    "        bar_err = df.iloc[indices]['stderr']\n",
    "        assert(all([x == df.iloc[indices[0]]['ens_method'] for x in df.iloc[indices]['ens_method']]))\n",
    "        ax.bar(bar_offsets[i], bar_height, width=bar_width, color=palette[i], #   yerr=bar_err,\n",
    "               label=df.iloc[indices[0]]['ens_method'], edgecolor=(0.5, 0.5, 0.5), capsize=5)\n",
    "        xticklabels.append(df.iloc[indices[0]]['train_method'].replace('_', '\\n'))\n",
    "    \n",
    "    ax.set_ylim([np.min(df['acc'])-1.0, np.max(df['acc'])+1.0])\n",
    "    # ax.legend(loc='upper left', prop={'size': 12})\n",
    "    # ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    ax.set_xticks(np.arange(1, n_groups+1))\n",
    "    ax.set_xticklabels(['Clean', 'Jpeg', 'Blur', 'Noise'], fontsize=14)\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('Accuracy', fontsize=16)\n",
    "    ax.set_title(attr.replace('_', ' '), fontsize=16)\n",
    "    for tick in ax.yaxis.get_major_ticks():\n",
    "        tick.label.set_fontsize(12) \n",
    "    \n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels() # on the last axis\n",
    "lgd = f.legend(handles, labels, loc='lower center', ncol=4, prop={'size': 12},\n",
    "               bbox_to_anchor=(0.5, -0.08), edgecolor='1.0')\n",
    "f.tight_layout()\n",
    "save(f, 'graph_face_untargeted_corruption.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(1, 4, figsize=(16, 3.5))\n",
    "\n",
    "for row, attr in enumerate(table_acc.index):\n",
    "    ax = axes[row]\n",
    "    df = {'train_method': ['Uncorrupted'] * 4 + ['FGSM'] * 4 + ['PGD'] * 4 + ['CW'] * 4,\n",
    "          'ens_method': ['Image', 'Reconstruction', 'Style-mix Ensemble', 'Combined Ensemble'] * 4,\n",
    "          'acc': table_acc.iloc[row, list(range(28, 32)) + list(range(44,56))],\n",
    "          'stderr': table_stderr.iloc[row, list(range(28, 32)) + list(range(44,56))]\n",
    "         }\n",
    "    df = pd.DataFrame.from_dict(df)\n",
    "    # display(df)\n",
    "    palette = make_blue_palette(4)\n",
    "    \n",
    "    group_size=4\n",
    "    n_groups=4\n",
    "    bar_width=0.2\n",
    "    bar_offsets = bar_offset(group_size, n_groups, bar_width)\n",
    "    xticklabels = []\n",
    "    for i in range(group_size):\n",
    "        indices = np.arange(i, n_groups*group_size, group_size)\n",
    "        bar_height = df.iloc[indices]['acc']\n",
    "        bar_err = df.iloc[indices]['stderr']\n",
    "        assert(all([x == df.iloc[indices[0]]['ens_method'] for x in df.iloc[indices]['ens_method']]))\n",
    "        b = ax.bar(bar_offsets[i], bar_height, width=bar_width, color=palette[i], # yerr=bar_err,\n",
    "                   label=df.iloc[indices[0]]['ens_method'], edgecolor=(0.5, 0.5, 0.5), capsize=5)\n",
    "        xticklabels.append(df.iloc[indices[0]]['train_method'].replace('_', '\\n'))\n",
    "    \n",
    "    # ax.set_ylim([np.min(df['acc'])-1.0, np.max(df['acc'])+1.0])\n",
    "    # ax.legend(loc='upper center', prop={'size': 12})\n",
    "    ax.set_ylim([0, 100])\n",
    "    #ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    ax.set_xticks(np.arange(1, n_groups+1))\n",
    "    ax.set_xticklabels(['Clean', 'FGSM', 'PGD', 'CW'], fontsize=14)\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('Accuracy', fontsize=16)\n",
    "    ax.set_title(attr.replace('_', ' '), fontsize=16)\n",
    "    for tick in ax.yaxis.get_major_ticks():\n",
    "        tick.label.set_fontsize(12) \n",
    "    \n",
    "# axes[0].legend([],[], frameon=False)\n",
    "# axes[1].legend([],[], frameon=False)\n",
    "# axes[2].legend([],[], frameon=False)\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels() # on the last axis\n",
    "lgd = f.legend(handles, labels, loc='lower center', ncol=4, prop={'size': 12}, bbox_to_anchor=(0.5, -0.08), edgecolor='1.0')\n",
    "f.tight_layout()\n",
    "save(f, 'graph_face_targeted_corruption.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stylegan ensemble size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_best_weight_ensemble_size(val_data_file, test_data_file, expt_name, verbose=True, add_aug=False, seed=None):\n",
    "    ens_sizes = [0, 2, 4, 8, 12, 16, 20, 24, 28, 30, 31]\n",
    "    num_samples = 16\n",
    "    assert('val' in val_data_file)\n",
    "    assert('test' in test_data_file)\n",
    "    # compute best val setting using full ensemble\n",
    "    val_accuracy_info = get_accuracy_from_npz(val_data_file, expt_name, add_aug=add_aug, ens_size=31, seed=seed)\n",
    "    val_ensemble_table = val_accuracy_info['ensemble_table']\n",
    "    # best_val_setting = val_ensemble_table.iloc[val_ensemble_table['acc'].idxmax(), :]\n",
    "    best_val_setting = val_ensemble_table.iloc[val_ensemble_table['acc'].argsort().iloc[-1], :]\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Val original %0.4f Val reconstructed %0.4f\" % \n",
    "              (val_accuracy_info['acc_original'], val_accuracy_info['acc_reconstructed']))\n",
    "        print(\"%0.4f @ %0.4f %s\" % (best_val_setting['acc'], best_val_setting['weight'], best_val_setting['expt_name']))\n",
    "    \n",
    "    \n",
    "    # test: iterate through ensemble sizes, taking samples from each\n",
    "    accs_reconstructed = []\n",
    "    accs_original = []\n",
    "    test_table = OrderedDict([(ens_size, []) for ens_size in ens_sizes])\n",
    "    for ens_size in ens_sizes:\n",
    "        for sample in range(num_samples):\n",
    "            test_accuracy_info = get_accuracy_from_npz(test_data_file, expt_name, weight=best_val_setting['weight'], \n",
    "                                                       add_aug=add_aug, ens_size=ens_size, seed=sample)\n",
    "\n",
    "            accs_reconstructed.append(test_accuracy_info['acc_reconstructed'])\n",
    "            accs_original.append(test_accuracy_info['acc_original'])\n",
    "            test_ensemble_table = test_accuracy_info['ensemble_table']\n",
    "            assert(test_ensemble_table.shape[0] == 1) # it should only evaluate at the specified weight\n",
    "            test_setting_from_val = test_ensemble_table.iloc[0, :]\n",
    "            test_table[ens_size].append(test_setting_from_val['acc'])\n",
    "            \n",
    "    # sanity check\n",
    "    assert(all([x == accs_reconstructed[0] for x in accs_reconstructed]))\n",
    "    assert(all([x == accs_original[0] for x in accs_original]))\n",
    "\n",
    "    test_df = pd.DataFrame.from_dict(test_table, orient='index', columns=range(num_samples))\n",
    "    \n",
    "    return {'val_info': val_accuracy_info, 'test_info': test_accuracy_info, \n",
    "            'val_setting': best_val_setting, 'test_df': test_df}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt_name = 'stylemix_fine'\n",
    "expt_data = [\n",
    "    ('Smiling', f'results/precomputed_evaluations/celebahq/output/%s_%s/gan_ensemble_stylemix_fine_tensortransform.npz'),\n",
    "    ('Arched_Eyebrows', f'results/precomputed_evaluations/celebahq/output/%s_%s/gan_ensemble_stylemix_fine_tensortransform.npz'),\n",
    "    ('Wavy_Hair', f'results/precomputed_evaluations/celebahq/output/%s_%s/gan_ensemble_stylemix_fine_tensortransform.npz'),\n",
    "    ('Young', f'results/precomputed_evaluations/celebahq/output/%s_%s/gan_ensemble_stylemix_fine_tensortransform.npz')\n",
    "]\n",
    "\n",
    "f, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "# axes = [ax for row in axes for ax in row]\n",
    "\n",
    "for i, (attr, data_file_base) in enumerate(expt_data):\n",
    "    ax = axes[i]\n",
    "    output = compute_best_weight_ensemble_size(data_file_base % (attr, 'val'), \n",
    "                                               data_file_base % (attr, 'test'), \n",
    "                                               expt_name)\n",
    "    plot_vals = output['test_df'].to_numpy()\n",
    "    m = np.mean(plot_vals, axis=1)\n",
    "    s = np.std(plot_vals, axis=1) / np.sqrt(plot_vals.shape[1])\n",
    "    ax.plot(output['test_df'].index, m)\n",
    "    ax.fill_between(output['test_df'].index, m-s, m+s, alpha=0.3)\n",
    "    ax.set_title(attr.replace('_', ' '), fontsize=16)\n",
    "    ax.set_xlabel('Number of\\nGAN samples', fontsize=14)\n",
    "    ax.set_ylabel('Accuracy', fontsize=16)\n",
    "    for tick in ax.yaxis.get_major_ticks():\n",
    "        tick.label.set_fontsize(12) \n",
    "    for tick in ax.xaxis.get_major_ticks():\n",
    "        tick.label.set_fontsize(12) \n",
    "    # ax.axhline(test_output[0][0])\n",
    "    # ax.axhline(test_output[2])\n",
    "f.tight_layout()\n",
    "save(f, 'graph_face_ensemble_size.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_dict = {}\n",
    "\n",
    "for classifier in ['imageclassifier', 'latentclassifier', 'latentclassifier_layer6', 'latentclassifier_layer7']:\n",
    "    print(\"==================\")\n",
    "    for expt_name in ['stylemix_layer6', 'stylemix_layer7']:\n",
    "        print(\"---> %s %s\" % (classifier, expt_name))\n",
    "        val_data_file = f'results/precomputed_evaluations/cifar10/output/{classifier}_val/gan_ensemble_{expt_name}.npz'\n",
    "        test_data_file = val_data_file.replace('_val', '_test')\n",
    "        resampled_accs = resample_wrapper(val_data_file, test_data_file, expt_name, ens_size=31, \n",
    "                                          add_aug=False, verbose=False)\n",
    "        \n",
    "        print(\"val improvement: %0.3f\" % (resampled_accs['val_avg'] - resampled_accs['val_acc_original']))\n",
    "        print(\"test improvement: %0.3f\" % (resampled_accs['test_avg'] - resampled_accs['test_acc_original']))\n",
    "        \n",
    "        oracle = get_accuracy_from_npz(test_data_file, expt_name)\n",
    "        oracle_table = oracle['ensemble_table']\n",
    "        oracle_setting = oracle_table.iloc[oracle_table['acc'].argsort().iloc[-1], :]\n",
    "        print(\"oracle imrovement: %0.3f\" % (oracle_setting['acc'] - resampled_accs['test_acc_original']))\n",
    "        if expt_name == 'stylemix_layer6':\n",
    "            # also extract the classifier acc on images\n",
    "            table_dict['%s %s' % (classifier, 'images')] = [np.nan, resampled_accs['val_acc_original'],\n",
    "                                                         resampled_accs['test_acc_original'], np.nan, np.nan]\n",
    "        table_dict['%s %s' % (classifier, expt_name)] = [np.mean(resampled_accs['weights']), resampled_accs['val_avg'],\n",
    "                                                         resampled_accs['test_avg'], oracle_setting['weight'],\n",
    "                                                         oracle_setting['acc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.DataFrame.from_dict(table_dict, orient='index', \n",
    "                               columns=['val weight', 'val acc', 'test acc', 'oracle weight', 'oracle acc'])\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot it\n",
    "f, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "\n",
    "group_size = 3\n",
    "bar_width=0.2\n",
    "n_groups = 4 # training configurations\n",
    "bar_offsets = bar_offset(group_size, n_groups, bar_width)\n",
    "palette = make_yellow_palette(2)[1:] + make_blue_palette(2)[1:] + make_green_palette(2)[1:]\n",
    "\n",
    "ind = 0.2\n",
    "# ax.axhline(im_crops['acc_ensembled'], color='k', linestyle=':', label='Original Images')\n",
    "ax.bar(bar_offsets[0], table.loc[[x for x in table.index if x.endswith('images')]]['test acc'],\n",
    "       width=bar_width, color=palette[0], label='Image', edgecolor=(0.5, 0.5, 0.5), capsize=5)\n",
    "\n",
    "for i, layer in enumerate([6, 7]):\n",
    "    ax.bar(bar_offsets[i+1], table.loc[[x for x in table.index if x.endswith('layer%d' % layer)]]['test acc'],\n",
    "           width=bar_width, color=palette[i+1], label='Style-mix Layer%d' % layer, edgecolor=(0.5, 0.5, 0.5), capsize=5)\n",
    "    \n",
    "ax.set_ylim([92, 96])\n",
    "ax.set_ylabel('Classification Accuracy', fontsize=14)\n",
    "ax.set_xticks(np.arange(1, n_groups+1))\n",
    "ax.legend()\n",
    "ax.set_xticklabels(['Original\\nImages', 'GAN\\nReconstructions',\n",
    "                    'Style-mix\\nLayer 6', 'Style-mix\\nLayer 8'], fontsize=12)\n",
    "ax.set_xlabel('Classifier training distribution', fontsize=16)\n",
    "save(f, 'graph_cifar10.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gan-ensembling",
   "language": "python",
   "name": "gan-ensembling"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}